{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CS 6476 Project 2: Local Feature Matching and Camera Calibration]()\n",
    "\n",
    "This iPython notebook:  \n",
    "(1) Loads and resizes images  \n",
    "(2) Finds interest points in those images                 (you code this)  \n",
    "(3) Describes each interest point with a local feature    (you code this)  \n",
    "(4) Naively finds matching features                       (you code this)  \n",
    "(5) Computes projection matrices from 3D to 2D points     (you code this)   \n",
    "(6) Determines camera fundamental matrices                (you code this)   \n",
    "(7) Matches interest points using epipolar geometry       (you code this)   \n",
    "(8) Visualizes the matches  \n",
    "(9) Evaluates the matches based on ground truth correspondences  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# from vision.utils import load_image, PIL_resize, rgb2gray, normalize_img, verify\n",
    "from vision.utils import (\n",
    "    verify,\n",
    "    evaluate_points,\n",
    "    visualize_points,\n",
    "    visualize_points_image,\n",
    "    plot3dview,\n",
    "    load_image,\n",
    "    PIL_resize,\n",
    "    rgb2gray,\n",
    "    normalize_img,\n",
    "    draw_epipolar_lines,\n",
    "    get_matches,\n",
    "    show_correspondence2,\n",
    ")\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Notre Dame\n",
    "image1 = load_image('./data/1a_notredame.jpg')\n",
    "image2 = load_image('./data/1b_notredame.jpg')\n",
    "eval_file = './ground_truth/notredame.pkl'\n",
    "\n",
    "# # Mount Rushmore -- this pair is relatively easy (still harder than Notre Dame, though)\n",
    "# image1 = load_image('./data/2a_rushmore.jpg')\n",
    "# image2 = load_image('./data/2b_rushmore.jpg')\n",
    "# eval_file = './ground_truth/rushmore.pkl'\n",
    "\n",
    "# # Episcopal Gaudi -- This pair is relatively difficult\n",
    "# image1 = load_image('./data/3a_gaudi.jpg')\n",
    "# image2 = load_image('./data/3b_gaudi.jpg')\n",
    "# eval_file = './ground_truth/gaudi.pkl'\n",
    "\n",
    "# # Your own image pair (for part 5) -- replace the name with your file name. Note that there is no eval_file.\n",
    "# image1 = load_image('./data/4a_myimage.jpg')\n",
    "# image2 = load_image('./data/4b_myimage.jpg')\n",
    "\n",
    "scale_factor = 0.5\n",
    "image1 = PIL_resize(image1, (int(image1.shape[1]*scale_factor), int(image1.shape[0]*scale_factor)))\n",
    "image2 = PIL_resize(image2, (int(image2.shape[1]*scale_factor), int(image2.shape[0]*scale_factor)))\n",
    "\n",
    "image1_bw = rgb2gray(image1)\n",
    "image2_bw = rgb2gray(image2)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(image1)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(image2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part 1: Harris Corner Detector \n",
    "## Find distinctive points in each image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Harris corner detector and SIFT rely heavily upon image gradient information. You'll implement `compute_image_gradients()` and then we'll visualize the magnitude of the image gradients. Which areas have highest mangitude, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.part1_harris_corner import compute_image_gradients\n",
    "from tests.test_part1_harris_corner import test_compute_image_gradients\n",
    "\n",
    "# print('compute_image_gradients(): ', verify(test_compute_image_gradients))\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.axis('off')\n",
    "\n",
    "Ix, Iy = compute_image_gradients(image1_bw)\n",
    "gradient_magnitudes = np.sqrt(Ix**2 + Iy**2)\n",
    "gradient_magnitudes = normalize_img(gradient_magnitudes)\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(r'$\\sqrt{I_x^2 + I_y^2}$')\n",
    "plt.imshow( (gradient_magnitudes*255).astype(np.uint8))\n",
    "\n",
    "Ix, Iy = compute_image_gradients(image2_bw)\n",
    "gradient_magnitudes = np.sqrt(Ix**2 + Iy**2)\n",
    "gradient_magnitudes = normalize_img(gradient_magnitudes)\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(r'$\\sqrt{I_x^2 + I_y^2}$')\n",
    "plt.imshow( (gradient_magnitudes*255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now compute the second moments $s_x^2, s_y^2, s_x s_y$ at each pixel, which aggregates gradient information in local neighborhoods. We'll use a 2d Gaussian filter to aggregate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.part1_harris_corner import second_moments\n",
    "sx2, sy2, sxsy = second_moments(image1_bw, ksize = 7, sigma = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare $s_x^2$, $s_y^2$, and $s_x s_y$ with $I_x$ and $I_y$, we see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.utils import normalize_img\n",
    "\n",
    "plt.figure(figsize=(12,9))\n",
    "Ix, Iy = compute_image_gradients(image1_bw)\n",
    "plt.subplot(2,3,1); plt.title(r'$I_x$')\n",
    "plt.imshow( (normalize_img(np.abs(Ix))*255).astype(np.uint8))\n",
    "plt.subplot(2,3,2); plt.title(r'$I_y$')\n",
    "plt.imshow( (normalize_img(np.abs(Iy))*255).astype(np.uint8))\n",
    "\n",
    "plt.subplot(2,3,4)\n",
    "plt.title(r'$s_x^2$')\n",
    "plt.imshow( (normalize_img(np.abs(sx2))*255).astype(np.uint8))\n",
    "\n",
    "plt.subplot(2,3,5)\n",
    "plt.title(r'$s_y^2$')\n",
    "plt.imshow( (normalize_img(np.abs(sy2))*255).astype(np.uint8))\n",
    "\n",
    "plt.subplot(2,3,6)\n",
    "plt.title(r'$s_xs_y$')\n",
    "plt.imshow( (normalize_img(np.abs(sxsy))*255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $s_xs_y$ is highest where there are both strong x-direction and y-direction gradients (corners and the central rose window).\n",
    "\n",
    "We'll now use these second moments to compute a \"cornerness score\" -- a corner response map -- as a function of these image gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.part1_harris_corner import compute_harris_response_map\n",
    "from tests.test_part1_harris_corner import test_compute_harris_response_map\n",
    "\n",
    "print('compute_harris_response_map(): ', verify(test_compute_harris_response_map))\n",
    "\n",
    "R = compute_harris_response_map(image1_bw)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(image1_bw, cmap='gray')\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(r'$R$')\n",
    "plt.imshow(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bright areas above are the areas of highest \"corners\".\n",
    "\n",
    "We'll now implement non-max suppression to find local maxima in the 2d response map. One simple way to do non-maximum suppression is to simply pick a local maximum over some window size (u, v). This can be achieved using max-pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to see the maxpool result\n",
    "from vision.part1_harris_corner import maxpool_numpy\n",
    "from tests.test_part1_harris_corner import test_maxpool_numpy, test_nms_maxpool_pytorch\n",
    "from vision.utils import verify\n",
    "\n",
    "# print('maxpool_numpy(): ', verify(test_maxpool_numpy))\n",
    "\n",
    "toy_response_map = np.array(\n",
    "[\n",
    "    [1,2,2,1,2],\n",
    "    [1,6,2,1,1],\n",
    "    [2,2,1,1,1],\n",
    "    [1,1,1,7,1],\n",
    "    [1,1,1,1,1]\n",
    "]).astype(np.float32)\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(toy_response_map.astype(np.uint8))\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "maxpooled_image = maxpool_numpy(toy_response_map, ksize=3)\n",
    "plt.imshow(maxpooled_image.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a simple 5x5 grid of response scores, non-max suppression will allow us to choose values that are local maxima. If we request the top $k=2$ responses of the toy response grid above, we should get (1,1) and (3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.part1_harris_corner import nms_maxpool_pytorch\n",
    "\n",
    "print('nms_maxpool_pytorch(): ', verify(test_nms_maxpool_pytorch))\n",
    "\n",
    "x_coords, y_coords, confidences = nms_maxpool_pytorch(toy_response_map, k=2, ksize=3)\n",
    "print('Coordinates of local maxima:')\n",
    "for x, y, c in zip(x_coords, y_coords, confidences):\n",
    "    print(f'\\tAt {x},{y}, local maximum w/ confidence={c:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will call the `get_harris_interest_points()` function in `part1_harris_corner.py` to detect 'interesting' points in the images. \n",
    "\n",
    "**IMPORTANT**\n",
    "Make sure to add your code in the `get_harris_interest_points()` function to call Harris Corner Detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.test_part1_harris_corner import test_get_harris_interest_points, test_remove_border_vals\n",
    "\n",
    "# print('test_remove_border_vals(): ', verify(test_remove_border_vals))\n",
    "\n",
    "print('get_harris_interest_points()', verify(test_get_harris_interest_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from vision.part1_harris_corner import get_harris_interest_points\n",
    "from vision.utils import show_interest_points\n",
    "\n",
    "num_interest_points = 2500\n",
    "X1, Y1, _ = get_harris_interest_points( copy.deepcopy(image1_bw), num_interest_points)\n",
    "X2, Y2, _ = get_harris_interest_points( copy.deepcopy(image2_bw), num_interest_points)\n",
    "\n",
    "num_pts_to_visualize = 300\n",
    "# Visualize the interest points\n",
    "rendered_img1 = show_interest_points(image1, X1[:num_pts_to_visualize], Y1[:num_pts_to_visualize])\n",
    "rendered_img2 = show_interest_points(image2, X2[:num_pts_to_visualize], Y2[:num_pts_to_visualize])\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1); plt.imshow(rendered_img1)\n",
    "plt.subplot(1,2,2); plt.imshow(rendered_img2)\n",
    "print(f'{len(X1)} corners in image 1, {len(X2)} corners in image 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Feature matching (Szeliski 7.1.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the simplest possible keypoint descriptor is to stack the 16x16 patch surrounding the keypoint into a 256-dimensional vector, and normalize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.utils import compute_normalized_patch_descriptors\n",
    "\n",
    "image1_features = compute_normalized_patch_descriptors(image1_bw, X1, Y1, feature_width=16)\n",
    "image2_features = compute_normalized_patch_descriptors(image2_bw, X2, Y2, feature_width=16)\n",
    "\n",
    "# Visualize what the first 300 feature vectors for image 1 look like (they should not be identical or all black)\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1); plt.imshow(image1_features[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test your feature matching implementation\n",
    "from tests.test_part2_feature_matching import (\n",
    "    test_match_features_ratio_test,\n",
    "    test_compute_feature_distances_2d,\n",
    "    test_compute_feature_distances_10d\n",
    ")\n",
    "print('compute_feature_distances (2d):', verify(test_compute_feature_distances_2d))\n",
    "print('compute_feature_distances (10d):', verify(test_compute_feature_distances_10d))\n",
    "print('match_features_ratio_test:', verify(test_match_features_ratio_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.part2_feature_matching import match_features_ratio_test\n",
    "\n",
    "matches, confidences = match_features_ratio_test(image1_features, image2_features)\n",
    "print('{:d} matches from {:d} corners'.format(len(matches), len(X1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "You might want to set 'num_pts_to_visualize' and 'num_pts_to_evaluate' to some constant (e.g. 100) once you start detecting hundreds of interest points, otherwise things might get too cluttered. You could also threshold based on confidence.  \n",
    "  \n",
    "There are two visualization functions below. You can comment out one of both of them if you prefer.\n",
    "\n",
    "NOTE: If you find that no matches are returned, you may encounter an error in the cells below. To avoid this, adjust your threshold in `match_features_ratio_test` to include at least one match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from vision.utils import show_correspondence_circles, show_correspondence_lines\n",
    "os.makedirs('./results', exist_ok=True)\n",
    "# num_pts_to_visualize = len(matches)\n",
    "num_pts_to_visualize = 200\n",
    "c1 = show_correspondence_circles(image1, image2,\n",
    "                    X1[matches[:num_pts_to_visualize, 0]], Y1[matches[:num_pts_to_visualize, 0]],\n",
    "                    X2[matches[:num_pts_to_visualize, 1]], Y2[matches[:num_pts_to_visualize, 1]])\n",
    "plt.figure(figsize=(10,5)); plt.imshow(c1)\n",
    "plt.savefig('./results/vis_circles.jpg', dpi=1000)\n",
    "c2 = show_correspondence_lines(image1, image2,\n",
    "                    X1[matches[:num_pts_to_visualize, 0]], Y1[matches[:num_pts_to_visualize, 0]],\n",
    "                    X2[matches[:num_pts_to_visualize, 1]], Y2[matches[:num_pts_to_visualize, 1]])\n",
    "plt.figure(figsize=(10,5)); plt.imshow(c2)\n",
    "plt.savefig('./results/vis_lines.jpg', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment out the function below if you are not testing on the Notre Dame, Episcopal Gaudi, and Mount Rushmore image pairs--this evaluation function will only work for those which have ground truth available.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.utils import evaluate_correspondence\n",
    "# num_pts_to_evaluate = len(matches)\n",
    "num_pts_to_evaluate = 2500\n",
    "_, c = evaluate_correspondence(image1, image2, eval_file, scale_factor,\n",
    "                        X1[matches[:num_pts_to_evaluate, 0]], Y1[matches[:num_pts_to_evaluate, 0]],\n",
    "                        X2[matches[:num_pts_to_evaluate, 1]], Y2[matches[:num_pts_to_evaluate, 1]])\n",
    "plt.figure(figsize=(8,4)); plt.imshow(c)\n",
    "plt.savefig('./results/eval.jpg', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Sift Feature Descriptor (Szeliski 7.1.2)\n",
    "SIFT relies upon computing the magnitudes and orientations of image gradients, and then computing weighted histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.test_part3_sift_descriptor import (\n",
    "    test_get_magnitudes_and_orientations,\n",
    "    test_get_gradient_histogram_vec_from_patch\n",
    ")\n",
    "print('get_magnitudes_and_orientations:', verify(test_get_magnitudes_and_orientations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('get_gradient_histogram_vec_from_patch():', verify(test_get_gradient_histogram_vec_from_patch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.test_part3_sift_descriptor import test_get_feat_vec, test_get_SIFT_descriptors\n",
    "print(verify(test_get_feat_vec))\n",
    "print(verify(test_get_SIFT_descriptors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.part3_sift_descriptor import get_SIFT_descriptors\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "image1_features = get_SIFT_descriptors(image1_bw, X1, Y1)\n",
    "image2_features = get_SIFT_descriptors(image2_bw, X2, Y2)\n",
    "end = time.time()\n",
    "duration = end - start\n",
    "print(f'SIFT took {duration} sec.')\n",
    "\n",
    "# visualize what the values of the first 200 SIFT feature vectors look like (should not be identical or all black)\n",
    "plt.figure(); plt.subplot(1,2,1); plt.imshow(image1_features[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from vision.utils import show_correspondence_circles, show_correspondence_lines\n",
    "\n",
    "matches, confidences = match_features_ratio_test(image1_features, image2_features)\n",
    "print('{:d} matches from {:d} corners'.format(len(matches), len(X1)))\n",
    "\n",
    "# num_pts_to_visualize = len(matches)\n",
    "num_pts_to_visualize = 200\n",
    "c1 = show_correspondence_circles(\n",
    "    image1,\n",
    "    image2,\n",
    "    X1[matches[:num_pts_to_visualize, 0]],\n",
    "    Y1[matches[:num_pts_to_visualize, 0]],\n",
    "    X2[matches[:num_pts_to_visualize, 1]],\n",
    "    Y2[matches[:num_pts_to_visualize, 1]]\n",
    ")\n",
    "plt.figure(figsize=(10,5)); plt.imshow(c1)\n",
    "plt.savefig('./results/vis_circles.jpg', dpi=1000)\n",
    "c2 = show_correspondence_lines(\n",
    "    image1,\n",
    "    image2,\n",
    "    X1[matches[:num_pts_to_visualize, 0]],\n",
    "    Y1[matches[:num_pts_to_visualize, 0]],\n",
    "    X2[matches[:num_pts_to_visualize, 1]],\n",
    "    Y2[matches[:num_pts_to_visualize, 1]]\n",
    ")\n",
    "plt.figure(figsize=(10,5)); plt.imshow(c2)\n",
    "plt.savefig('./results/vis_lines.jpg', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.utils import evaluate_correspondence\n",
    "num_pts_to_evaluate = len(matches)\n",
    "_, c = evaluate_correspondence(\n",
    "    image1,\n",
    "    image2,\n",
    "    eval_file,\n",
    "    scale_factor,\n",
    "    X1[matches[:num_pts_to_evaluate, 0]],\n",
    "    Y1[matches[:num_pts_to_evaluate, 0]],\n",
    "    X2[matches[:num_pts_to_evaluate, 1]],\n",
    "    Y2[matches[:num_pts_to_evaluate, 1]]\n",
    ")\n",
    "plt.figure(figsize=(8,4)); plt.imshow(c)\n",
    "plt.savefig('./results/eval.jpg', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure your code runs in under 90 sec and achieves >80% acc on the Notre Dame pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.test_part3_sift_descriptor import (\n",
    "    test_feature_matching_speed,\n",
    "    test_feature_matching_accuracy\n",
    ")\n",
    "print('SIFT pipeline speed test:', verify(test_feature_matching_speed))\n",
    "print('SIFT pipeline accuracy test:', verify(test_feature_matching_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Camera Projection Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "img_path = \"data/CCB_GaTech/pic_a.jpg\"\n",
    "points_2d = np.loadtxt(\"data/CCB_GaTech/pts2d-norm-pic_a.txt\")\n",
    "points_3d = np.loadtxt(\"data/CCB_GaTech/pts3d-norm.txt\")\n",
    "\n",
    "# (Optional) Uncomment these four lines once you have your code working with the easier, normalized points above.\n",
    "# points_2d = np.loadtxt('../data/CCB_GaTech/pts2d-pic_b.txt')\n",
    "# points_3d = np.loadtxt('../data/CCB_GaTech/pts3d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the projection matrix given corresponding 2D & 3D points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.part4_projection_matrix import (\n",
    "    calculate_projection_matrix,\n",
    "    calculate_camera_center,\n",
    ")\n",
    "\n",
    "from tests.test_part4_projection_matrix import (\n",
    "    test_projection,\n",
    "    test_calculate_projection_matrix,\n",
    "    test_calculate_camera_center,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"projection():\", verify(test_projection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = calculate_projection_matrix(points_2d, points_3d)\n",
    "print(\"The projection matrix is\\n\", M)\n",
    "\n",
    "[projected_2d_pts, residual] = evaluate_points(M, points_2d, points_3d)\n",
    "print(\"The total residual is {:f}\".format(residual))\n",
    "plt.figure()\n",
    "plt.imshow(load_image(img_path))\n",
    "visualize_points(points_2d, projected_2d_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"calculate_projection_matrix():\", verify(test_calculate_projection_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the camera center using M found from the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center = calculate_camera_center(M)\n",
    "print(\n",
    "    \"The estimated location of the camera is <{:.4f}, {:.4f}, {:.4f}>\".format(*center)\n",
    ")\n",
    "plt.figure()\n",
    "plt.imshow(load_image(img_path))\n",
    "ax = plot3dview(points_3d, center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test_calculate_camera_center():\", verify(test_calculate_camera_center))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera Calibration for Argoverse image data\n",
    "We'll now estimate the position of a camera mounted on an autonomous vehicle, using data from Argoverse. We'll use images from the \"ring front center\" camera, which faces forward.\n",
    "\n",
    "\n",
    "<img src=\"https://www.argoverse.org/assets/images/reference_images/O2V4_vehicle_annotation.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argoverse Scene 3\n",
    "img_path = \"data/argoverse_log_d60558d2_pair3/pic3.jpg\"\n",
    "points_2d = np.loadtxt(\"data/argoverse_log_d60558d2_pair3/points_2d.txt\")\n",
    "points_3d = np.loadtxt(\"data/argoverse_log_d60558d2_pair3/points_3d.txt\")\n",
    "# # # Argoverse Scene 2\n",
    "# img_path = '../data/argoverse_log_d60558d2_pair2/pic2.jpg'\n",
    "# points_2d = np.loadtxt('../data/argoverse_log_d60558d2_pair2/points_2d.txt')\n",
    "# points_3d = np.loadtxt('../data/argoverse_log_d60558d2_pair2/points_3d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = calculate_projection_matrix(points_2d, points_3d)\n",
    "print(\"The projection matrix is\\n\", M)\n",
    "\n",
    "[projected_2d_pts, residual] = evaluate_points(M, points_2d, points_3d)\n",
    "print(\"The total residual is {:f}\".format(residual))\n",
    "plt.figure()\n",
    "plt.imshow(load_image(img_path))\n",
    "visualize_points(points_2d, projected_2d_pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these 2d-3d pairs, the \"world\" frame is defined as the \"ego-vehicle\" frame, where the origin is at the center of the back axle of the vehicle.\n",
    "\n",
    "Thus, if your camera center estimate is correct, it should tell you how far to move forward (+x) and how far to move left (+y) and move up (+z) to reach teh camera's position.\n",
    "\n",
    "\n",
    "The \"egovehicle\" coordinate system and \"camera\" coordinate system:\n",
    "<img width=\"300\"  src=\"https://user-images.githubusercontent.com/16724970/108759169-034e6180-751a-11eb-8a06-fbe344f1ee68.png\">\n",
    "<img width=\"300\" src=\"https://user-images.githubusercontent.com/16724970/108759182-06495200-751a-11eb-8162-8b17f9cdee4b.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center = calculate_camera_center(M)\n",
    "print(\n",
    "    \"The estimated location of the camera is <{:.4f}, {:.4f}, {:.4f}>\".format(*center)\n",
    ")\n",
    "plt.figure()\n",
    "plt.imshow(load_image(img_path))\n",
    "ax = plot3dview(points_3d, center)\n",
    "ax.view_init(elev=15, azim=180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Fundamental Matrix Estimation\n",
    "We'll now solve for the Fundamental Matrix by implementing [Hartley's 8-Point algorithm](https://www.cse.unr.edu/~bebis/CS485/Handouts/hartley.pdf).\n",
    "\n",
    "Please visualize the results for both Gaudi and Notre Dame for your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.part5_fundamental_matrix import estimate_fundamental_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "# Notre Dame\n",
    "points_2d_pic_a = np.loadtxt(\"data/Notre_Dame/pts2d-pic_a.txt\")\n",
    "points_2d_pic_b = np.loadtxt(\"data/Notre_Dame/pts2d-pic_b.txt\")\n",
    "img_a = load_image(\"data/Notre_Dame/921919841_a30df938f2_o.jpg\")\n",
    "img_b = load_image(\"data/Notre_Dame/4191453057_c86028ce1f_o.jpg\")\n",
    "\n",
    "# Gaudi\n",
    "# points_2d_pic_a = np.loadtxt(\"data/Episcopal_Gaudi/pts2d-pic_a.txt\")\n",
    "# points_2d_pic_b = np.loadtxt(\"data/Episcopal_Gaudi/pts2d-pic_b.txt\")\n",
    "# img_a = load_image('data/Episcopal_Gaudi/3743214471_1b5bbfda98_o.jpg');\n",
    "# img_b = load_image('data/Episcopal_Gaudi/4386465943_8cf9776378_o.jpg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate fundamental matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.test_part5_fundamental_matrix import (\n",
    "    test_estimate_fundamental_matrix,\n",
    "    test_normalize_points,\n",
    "    test_unnormalize_F,\n",
    ")\n",
    "\n",
    "print(\"test_estimate_fundamental_matrix():\", verify(test_estimate_fundamental_matrix))\n",
    "print(\"test_normalize_points():\", verify(test_normalize_points))\n",
    "print(\"test_unnormalize_F():\", verify(test_unnormalize_F))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = estimate_fundamental_matrix(points_2d_pic_a, points_2d_pic_b)\n",
    "\n",
    "# Draw epipolar lines using the fundamental matrix\n",
    "draw_epipolar_lines(F, img_a, img_b, points_2d_pic_a, points_2d_pic_b, figsize=(13, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Fundamental Matrix with RANSAC (Szeliski 6.1.4)\n",
    "\n",
    "**Mount Rushmore**: This pair is easy, and most of the initial matches are correct. The base fundamental matrix estimation without coordinate normalization will work fine with RANSAC. \n",
    "\n",
    "**Notre Dame**: This pair is difficult because the keypoints are largely on the same plane. Still, even an inaccurate fundamental matrix can do a pretty good job of filtering spurious matches.  \n",
    "\n",
    "**Gaudi**: This pair is difficult and doesn't find many correct matches unless you run at high resolution, but that will lead to tens of thousands of SIFT features, which will be somewhat slow to process. Normalizing the coordinates seems to make this pair work much better.  \n",
    "\n",
    "**Woodruff**: This pair has a clearer relationship between the cameras (they are converging and have a wide baseline between them). The estimated fundamental matrix is less ambiguous and you should get epipolar lines qualitatively similar to part 2 of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.part6_ransac import (\n",
    "    calculate_num_ransac_iterations,\n",
    "    ransac_fundamental_matrix,\n",
    ")\n",
    "from vision.utils import single2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# Mount Rushmore\n",
    "pic_a = single2im(load_image('data/Mount_Rushmore/9193029855_2c85a50e91_o.jpg'))\n",
    "scale_a = 0.25\n",
    "pic_b = single2im(load_image('data/Mount_Rushmore/7433804322_06c5620f13_o.jpg'))\n",
    "scale_b = 0.37\n",
    "n_feat = 5e4\n",
    "\n",
    "# Notre Dame\n",
    "# pic_a = single2im(load_image(\"data/Notre_Dame/921919841_a30df938f2_o.jpg\"))\n",
    "# scale_a = 0.5\n",
    "# pic_b = single2im(load_image(\"data/Notre_Dame/4191453057_c86028ce1f_o.jpg\"))\n",
    "# scale_b = 0.5\n",
    "# n_feat = 4e3\n",
    "\n",
    "# Gaudi\n",
    "# pic_a = single2im(load_image('data/Episcopal_Gaudi/3743214471_1b5bbfda98_o.jpg')); scale_a = 0.8\n",
    "# pic_b = single2im(load_image('data/Episcopal_Gaudi/4386465943_8cf9776378_o.jpg')); scale_b = 1.0\n",
    "# n_feat = 2e4\n",
    "\n",
    "# Woodruff\n",
    "# pic_a = single2im(load_image('data/Woodruff_Dorm/wood1.jpg')); scale_a = 0.65\n",
    "# pic_b = single2im(load_image('data/Woodruff_Dorm/wood2.jpg')); scale_b = 0.65\n",
    "# n_feat = 5e4\n",
    "\n",
    "pic_a = cv2.resize(pic_a, None, fx=scale_a, fy=scale_a)\n",
    "pic_b = cv2.resize(pic_b, None, fx=scale_b, fy=scale_b)\n",
    "print(pic_a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds matching points in the two images using OpenCV's implementation of SIFT.\n",
    "# There can still be many spurious matches, though.\n",
    "points_2d_pic_a, points_2d_pic_b = get_matches(pic_a, pic_b, n_feat)\n",
    "print(\"Found {:d} possibly matching features\".format(len(points_2d_pic_a)))\n",
    "match_image = show_correspondence2(\n",
    "    pic_a,\n",
    "    pic_b,\n",
    "    points_2d_pic_a[:, 0],\n",
    "    points_2d_pic_a[:, 1],\n",
    "    points_2d_pic_b[:, 0],\n",
    "    points_2d_pic_b[:, 1],\n",
    ")\n",
    "plt.figure()\n",
    "plt.imshow(match_image)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Fundamental Matrix using RANSAC\n",
    "Compare your results on the Notre Dame image pair below to your results from Part 2. How accurate do the point correspondences look now? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F, matched_points_a, matched_points_b = ransac_fundamental_matrix(\n",
    "    points_2d_pic_a, points_2d_pic_b\n",
    ")\n",
    "\n",
    "# Draw the epipolar lines on the images and corresponding matches\n",
    "match_image = show_correspondence2(\n",
    "    pic_a,\n",
    "    pic_b,\n",
    "    matched_points_a[:, 0],\n",
    "    matched_points_a[:, 1],\n",
    "    matched_points_b[:, 0],\n",
    "    matched_points_b[:, 1],\n",
    ")\n",
    "plt.figure()\n",
    "plt.imshow(match_image)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_epipolar_lines(\n",
    "    F, pic_a, pic_b, matched_points_a, matched_points_b, figsize=(12, 8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.test_part6_ransac import (\n",
    "    test_calculate_num_ransac_iterations,\n",
    "    test_ransac_fundamental_matrix,\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"test_calculate_num_ransac_iterations():\",\n",
    "    verify(test_calculate_num_ransac_iterations),\n",
    ")\n",
    "print(\"test_ransac_fundamental_matrix():\", verify(test_ransac_fundamental_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison\n",
    "We'll now test the quality of Fundamental matrices we can compute with and without RANSAC on an image pair from the [Argoverse](https://www.argoverse.org/) autonomous driving dataset. Does RANSAC improve the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.part6_ransac import (\n",
    "    calculate_num_ransac_iterations,\n",
    "    ransac_fundamental_matrix,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_a = single2im(load_image(\n",
    "    \"data/argoverse_log_273c1883/ring_front_center_315975640448534784.jpg\"\n",
    "))\n",
    "scale_a = 0.5\n",
    "pic_b = single2im(load_image(\n",
    "    \"data/argoverse_log_273c1883/ring_front_center_315975643412234000.jpg\"\n",
    "))\n",
    "scale_b = 0.5\n",
    "\n",
    "n_feat = 4e3\n",
    "num_matches_to_plot = 50\n",
    "\n",
    "pic_a = cv2.resize(pic_a, None, fx=scale_a, fy=scale_a)\n",
    "pic_b = cv2.resize(pic_b, None, fx=scale_b, fy=scale_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_2d_pic_a, points_2d_pic_b = get_matches(pic_a, pic_b, n_feat)\n",
    "\n",
    "print(\"Found {:d} possibly matching features\".format(len(points_2d_pic_a)))\n",
    "match_image = show_correspondence2(\n",
    "    pic_a,\n",
    "    pic_b,\n",
    "    points_2d_pic_a[:num_matches_to_plot, 0],\n",
    "    points_2d_pic_a[:num_matches_to_plot, 1],\n",
    "    points_2d_pic_b[:num_matches_to_plot, 0],\n",
    "    points_2d_pic_b[:num_matches_to_plot, 1],\n",
    ")\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.imshow(match_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without RANSAC Estimation\n",
    "If we ignore RANSAC and use only our implementation from Part2, we get the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_wo_ransac = estimate_fundamental_matrix(points_2d_pic_a, points_2d_pic_b)\n",
    "\n",
    "# Draw epipolar lines using the fundamental matrix\n",
    "draw_epipolar_lines(\n",
    "    F_wo_ransac, pic_a, pic_b, points_2d_pic_a, points_2d_pic_b, figsize=(13, 4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Ransac Estimation\n",
    "Now we'll use our RANSAC implementation from Part 3. Where does the epipole fall in the left image? (think about what it represents). The camera is mounted on an autonomous vehicle identical to the vehicle seen up ahead in the left image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "F, matched_points_a, matched_points_b = ransac_fundamental_matrix(\n",
    "    points_2d_pic_a, points_2d_pic_b\n",
    ")\n",
    "\n",
    "draw_epipolar_lines(\n",
    "    F, pic_a, pic_b, matched_points_a, matched_points_b, figsize=(13, 4)\n",
    ")\n",
    "\n",
    "match_image = show_correspondence2(\n",
    "    pic_a,\n",
    "    pic_b,\n",
    "    points_2d_pic_a[:num_matches_to_plot, 0],\n",
    "    points_2d_pic_a[:num_matches_to_plot, 1],\n",
    "    points_2d_pic_b[:num_matches_to_plot, 0],\n",
    "    points_2d_pic_b[:num_matches_to_plot, 1],\n",
    ")\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.imshow(match_image)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e098c37a6bc51c15d5fe2ca7e7edae220231c9af2d0d38bc1af2f83259f282af"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
